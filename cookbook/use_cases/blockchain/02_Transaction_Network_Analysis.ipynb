{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Hawksight-AI/semantica/blob/main/cookbook/use_cases/blockchain/02_Transaction_Network_Analysis.ipynb)\n",
        "\n",
        "# Transaction Network Analysis - Pattern Detection & Graph Analytics\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook demonstrates **blockchain transaction network analysis** using Semantica with focus on **pattern detection**, **network analytics**, and **real-time processing**. The pipeline analyzes blockchain transaction networks to detect patterns, identify whale movements, and analyze token flows.\n",
        "\n",
        "### Key Features\n",
        "\n",
        "- **Pattern Detection**: Emphasizes graph analytics for transaction pattern recognition\n",
        "- **Network Analytics**: Uses centrality measures and community detection\n",
        "- **Temporal Analysis**: Time-aware queries and transaction evolution tracking\n",
        "- **Whale Tracking**: Identifies large transaction movements\n",
        "- **Flow Analysis**: Analyzes token flows through the network\n",
        "- **Comprehensive Data Sources**: Multiple blockchain APIs, analytics platforms, and databases\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "- Ingest blockchain transaction data from multiple sources\n",
        "- Extract transaction entities (Transactions, Wallets, Addresses, Blocks, Flows)\n",
        "- Build temporal transaction network graphs\n",
        "- Perform graph analytics (centrality, communities, connectivity)\n",
        "- Detect patterns and whale movements\n",
        "- Analyze token flows and transaction paths\n",
        "- Store and query transaction data using vector stores and graph stores\n",
        "\n",
        "### Pipeline Flow\n",
        "\n",
        "```mermaid\n",
        "graph TD\n",
        "    A[Data Ingestion] --> B[Document Parsing]\n",
        "    B --> C[Text Processing]\n",
        "    C --> D[Entity Extraction]\n",
        "    D --> E[Relationship Extraction]\n",
        "    E --> F[Deduplication]\n",
        "    F --> G[Conflict Detection]\n",
        "    G --> H[Transaction Network Graph]\n",
        "    H --> I[Embeddings]\n",
        "    I --> J[Vector Store]\n",
        "    H --> K[Graph Analytics]\n",
        "    K --> L[Temporal Queries]\n",
        "    L --> M[Pattern Detection]\n",
        "    M --> N[Flow Analysis]\n",
        "    J --> O[GraphRAG Queries]\n",
        "    H --> P[Graph Store]\n",
        "    O --> Q[Visualization]\n",
        "    P --> Q\n",
        "    Q --> R[Export]\n",
        "```\n",
        "\n",
        "## Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution ~gno (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~lotly (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ython-socketio (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~gno (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~lotly (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ython-socketio (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~gno (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~lotly (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ython-socketio (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n"
          ]
        }
      ],
      "source": [
        "%pip install -qU semantica networkx matplotlib plotly pandas faiss-cpu beautifulsoup4 groq sentence-transformers scikit-learn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration & Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\", \"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\")\n",
        "\n",
        "# Configuration constants\n",
        "EMBEDDING_DIMENSION = 384\n",
        "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
        "CHUNK_SIZE = 1000\n",
        "CHUNK_OVERLAP = 200\n",
        "TEMPORAL_GRANULARITY = \"day\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ingesting Blockchain Transaction Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ingesting from 8 RSS feed sources...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div style='font-family: monospace;'><h4>üß† Semantica - üìä Current Progress</h4><table style='width: 100%; border-collapse: collapse;'><tr><th>Status</th><th>Action</th><th>Module</th><th>Submodule</th><th>File</th><th>Time</th></tr><tr><td>‚ùå</td><td>Semantica is parsing</td><td>üîç parse</td><td>DocumentParser</td><td>p>\n",
              "</td><td>0.00s</td></tr><tr><td>‚ùå</td><td>Semantica is parsing</td><td>üîç parse</td><td>DocumentParser</td><td>p>\n",
              "</td><td>0.00s</td></tr><tr><td>‚ùå</td><td>Semantica is parsing</td><td>üîç parse</td><td>DocumentParser</td><td>p>\n",
              "</td><td>0.00s</td></tr><tr><td>‚ùå</td><td>Semantica is parsing</td><td>üîç parse</td><td>DocumentParser</td><td>p>\n",
              "</td><td>0.00s</td></tr><tr><td>‚ùå</td><td>Semantica is parsing</td><td>üîç parse</td><td>DocumentParser</td><td>p>\n",
              "</td><td>0.02s</td></tr><tr><td>‚úÖ</td><td>Semantica is normalizing</td><td>üîß normalize</td><td>TextNormalizer</td><td>-</td><td>0.00s</td></tr><tr><td>üîÑ</td><td>Semantica is extracting</td><td>üéØ semantic_extract</td><td>NERExtractor</td><td>-</td><td>0.00s</td></tr><tr><td>‚úÖ</td><td>Semantica is extracting</td><td>üéØ semantic_extract</td><td>RelationExtractor</td><td>-</td><td>0.57s</td></tr><tr><td>‚úÖ</td><td>Semantica is resolving</td><td>‚ö†Ô∏è conflicts</td><td>ConflictDetector</td><td>-</td><td>0.01s</td></tr><tr><td>üîÑ</td><td>Semantica is building</td><td>üß† kg</td><td>GraphBuilder</td><td>-</td><td>630.57s</td></tr></table></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [1/8] CoinDesk: 25 documents\n",
            "  [2/8] CoinTelegraph: 30 documents\n",
            "  [3/8] Decrypt: 51 documents\n",
            "  [4/8] The Block: 19 documents\n",
            "  [5/8] CryptoSlate: 10 documents\n",
            "  [6/8] CryptoNews: 20 documents\n",
            "\n",
            "Ingesting from 3 web sources...\n",
            "  Warning: Blockchain.com Stats failed: 'WebIngestor' object has no attribute 'ingest'\n",
            "  Warning: Etherscan failed: 'WebIngestor' object has no attribute 'ingest'\n",
            "\n",
            "‚úÖ Total ingested: 155 documents\n",
            "   Sources: 6 unique sources\n",
            "   Source list: CoinDesk, CoinTelegraph, CryptoNews, CryptoSlate, Decrypt, The Block\n"
          ]
        }
      ],
      "source": [
        "from semantica.ingest import WebIngestor, FileIngestor, FeedIngestor\n",
        "import os\n",
        "from contextlib import redirect_stderr\n",
        "from io import StringIO\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "# Blockchain and crypto news RSS feeds\n",
        "feed_sources = [\n",
        "    (\"CoinDesk\", \"https://www.coindesk.com/arc/outboundfeeds/rss/\"),\n",
        "    (\"CoinTelegraph\", \"https://cointelegraph.com/rss\"),\n",
        "    (\"Decrypt\", \"https://decrypt.co/feed\"),\n",
        "    (\"The Block\", \"https://www.theblock.co/rss.xml\"),\n",
        "    (\"CryptoSlate\", \"https://cryptoslate.com/feed/\"),\n",
        "    (\"CryptoNews\", \"https://cryptonews.com/news/feed/\"),\n",
        "    (\"Bitcoin Magazine\", \"https://bitcoinmagazine.com/.rss/full/\"),\n",
        "    (\"Ethereum News\", \"https://ethereum.org/en/feed.xml\"),\n",
        "]\n",
        "\n",
        "# Blockchain data and analytics web sources\n",
        "web_sources = [\n",
        "    (\"Blockchain.com Stats\", \"https://www.blockchain.com/explorer\"),\n",
        "    (\"Etherscan\", \"https://etherscan.io/\"),\n",
        "    (\"Bitcoin Explorer\", \"https://blockstream.info/\"),\n",
        "]\n",
        "\n",
        "# Initialize ingestors\n",
        "feed_ingestor = FeedIngestor()\n",
        "web_ingestor = WebIngestor()\n",
        "all_documents = []\n",
        "\n",
        "# Ingest from RSS feeds\n",
        "print(f\"Ingesting from {len(feed_sources)} RSS feed sources...\")\n",
        "for i, (feed_name, feed_url) in enumerate(feed_sources, 1):\n",
        "    try:\n",
        "        with redirect_stderr(StringIO()):\n",
        "            feed_data = feed_ingestor.ingest_feed(feed_url, validate=False)\n",
        "        \n",
        "        feed_count = 0\n",
        "        for item in feed_data.items:\n",
        "            if not item.content:\n",
        "                item.content = item.description or item.title or \"\"\n",
        "            if item.content:\n",
        "                if not hasattr(item, 'metadata'):\n",
        "                    item.metadata = {}\n",
        "                item.metadata['source'] = feed_name\n",
        "                item.metadata['type'] = 'feed'\n",
        "                all_documents.append(item)\n",
        "                feed_count += 1\n",
        "        \n",
        "        if feed_count > 0:\n",
        "            print(f\"  [{i}/{len(feed_sources)}] {feed_name}: {feed_count} documents\")\n",
        "    except Exception as e:\n",
        "        if i <= 3:  # Show first few errors\n",
        "            print(f\"  Warning: {feed_name} failed: {str(e)[:50]}\")\n",
        "        continue\n",
        "\n",
        "# Ingest from web sources (transaction-related pages)\n",
        "print(f\"\\nIngesting from {len(web_sources)} web sources...\")\n",
        "for i, (web_name, web_url) in enumerate(web_sources, 1):\n",
        "    try:\n",
        "        with redirect_stderr(StringIO()):\n",
        "            web_documents = web_ingestor.ingest(web_url, method=\"url\")\n",
        "        \n",
        "        web_count = 0\n",
        "        for doc in web_documents:\n",
        "            if not hasattr(doc, 'metadata'):\n",
        "                doc.metadata = {}\n",
        "            doc.metadata['source'] = web_name\n",
        "            doc.metadata['type'] = 'web'\n",
        "            all_documents.append(doc)\n",
        "            web_count += 1\n",
        "        \n",
        "        if web_count > 0:\n",
        "            print(f\"  [{i}/{len(web_sources)}] {web_name}: {web_count} documents\")\n",
        "    except Exception as e:\n",
        "        if i <= 2:  # Show first few errors\n",
        "            print(f\"  Warning: {web_name} failed: {str(e)[:50]}\")\n",
        "        continue\n",
        "\n",
        "# Fallback to sample transaction data if no documents ingested\n",
        "if not all_documents:\n",
        "    print(\"\\n‚ö†Ô∏è No documents ingested from feeds/web sources. Using sample transaction data...\")\n",
        "    tx_data = \"\"\"\n",
        "    Transaction 0x123 transfers 1000 ETH from wallet 0xABC to wallet 0xDEF at block 18500000.\n",
        "    Transaction 0x456 transfers 500 BTC from wallet 0xGHI to wallet 0xJKL at block 18500001.\n",
        "    Large transaction 0x789 moves 10000 ETH (whale movement) from wallet 0xMNO to wallet 0xPQR at block 18500002.\n",
        "    Transaction 0xabc transfers 200 USDT from wallet 0xSTU to wallet 0xVWX at block 18500003.\n",
        "    Transaction 0xdef transfers 5000 ETH from wallet 0xYZA to wallet 0xBCD at block 18500004.\n",
        "    Transaction 0x111 transfers 3000 DAI from wallet 0xEFG to wallet 0xHIJ at block 18500005.\n",
        "    Transaction 0x222 transfers 1500 USDC from wallet 0xKLM to wallet 0xNOP at block 18500006.\n",
        "    Transaction 0x333 transfers 2500 LINK from wallet 0xQRS to wallet 0xTUV at block 18500007.\n",
        "    Transaction 0x444 transfers 8000 MATIC from wallet 0xWXY to wallet 0xZAB at block 18500008.\n",
        "    Transaction 0x555 transfers 12000 UNI from wallet 0xCDE to wallet 0xFGH at block 18500009.\n",
        "    \"\"\"\n",
        "    with open(\"data/transactions.txt\", \"w\") as f:\n",
        "        f.write(tx_data)\n",
        "    file_ingestor = FileIngestor()\n",
        "    all_documents = file_ingestor.ingest(\"data/transactions.txt\")\n",
        "    for doc in all_documents:\n",
        "        if not hasattr(doc, 'metadata'):\n",
        "            doc.metadata = {}\n",
        "        doc.metadata['source'] = 'Sample Data'\n",
        "        doc.metadata['type'] = 'sample'\n",
        "\n",
        "documents = all_documents\n",
        "\n",
        "# Count unique sources properly handling different document types\n",
        "unique_sources = set()\n",
        "for d in documents:\n",
        "    if hasattr(d, 'metadata') and d.metadata:\n",
        "        source = d.metadata.get('source', 'Unknown')\n",
        "        unique_sources.add(source)\n",
        "    elif isinstance(d, dict) and 'metadata' in d:\n",
        "        source = d['metadata'].get('source', 'Unknown')\n",
        "        unique_sources.add(source)\n",
        "\n",
        "print(f\"\\n‚úÖ Total ingested: {len(documents)} documents\")\n",
        "print(f\"   Sources: {len(unique_sources)} unique sources\")\n",
        "if unique_sources:\n",
        "    print(f\"   Source list: {', '.join(sorted(unique_sources))}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parsing Transaction Documents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parsing 155 documents...\n",
            "  Parsed 50/155 documents...\n",
            "  Parsed 100/155 documents...\n",
            "  Parsed 150/155 documents...\n",
            "  Parsed 155/155 documents...\n"
          ]
        }
      ],
      "source": [
        "from semantica.parse import DocumentParser\n",
        "\n",
        "parser = DocumentParser()\n",
        "\n",
        "print(f\"Parsing {len(documents)} documents...\")\n",
        "parsed_documents = []\n",
        "for i, doc in enumerate(documents, 1):\n",
        "    try:\n",
        "        parsed = parser.parse(\n",
        "            doc.content if hasattr(doc, 'content') else str(doc),\n",
        "            content_type=\"text\"\n",
        "        )\n",
        "        parsed_documents.append(parsed)\n",
        "    except Exception:\n",
        "        parsed_documents.append(doc)\n",
        "    if i % 50 == 0 or i == len(documents):\n",
        "        print(f\"  Parsed {i}/{len(documents)} documents...\")\n",
        "\n",
        "documents = parsed_documents\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Normalizing and Chunking Transaction Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normalizing 155 documents...\n",
            "  Normalized 50/155 documents...\n",
            "  Normalized 100/155 documents...\n",
            "  Normalized 150/155 documents...\n",
            "  Normalized 155/155 documents...\n",
            "Chunking 155 documents...\n",
            "  Chunked 50/155 documents (50 chunks so far)\n",
            "  Chunked 100/155 documents (101 chunks so far)\n",
            "  Chunked 150/155 documents (151 chunks so far)\n",
            "  Chunked 155/155 documents (156 chunks so far)\n",
            "Created 156 chunks from 155 documents\n"
          ]
        }
      ],
      "source": [
        "from semantica.normalize import TextNormalizer\n",
        "from semantica.split import TextSplitter\n",
        "\n",
        "normalizer = TextNormalizer()\n",
        "splitter = TextSplitter(\n",
        "    method=\"entity_aware\",\n",
        "    ner_method=\"spacy\",\n",
        "    chunk_size=CHUNK_SIZE,\n",
        "    chunk_overlap=CHUNK_OVERLAP\n",
        ")\n",
        "\n",
        "print(f\"Normalizing {len(documents)} documents...\")\n",
        "normalized_documents = []\n",
        "for i, doc in enumerate(documents, 1):\n",
        "    normalized_text = normalizer.normalize(\n",
        "        doc.content if hasattr(doc, 'content') else str(doc),\n",
        "        clean_html=True,\n",
        "        normalize_entities=True,\n",
        "        remove_extra_whitespace=True,\n",
        "        lowercase=False\n",
        "    )\n",
        "    normalized_documents.append(normalized_text)\n",
        "    if i % 50 == 0 or i == len(documents):\n",
        "        print(f\"  Normalized {i}/{len(documents)} documents...\")\n",
        "\n",
        "print(f\"Chunking {len(normalized_documents)} documents...\")\n",
        "chunked_documents = []\n",
        "for i, doc_text in enumerate(normalized_documents, 1):\n",
        "    try:\n",
        "        with redirect_stderr(StringIO()):\n",
        "            chunks = splitter.split(doc_text)\n",
        "        chunked_documents.extend(chunks)\n",
        "    except Exception:\n",
        "        simple_splitter = TextSplitter(method=\"recursive\", chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
        "        chunks = simple_splitter.split(doc_text)\n",
        "        chunked_documents.extend(chunks)\n",
        "    if i % 50 == 0 or i == len(normalized_documents):\n",
        "        print(f\"  Chunked {i}/{len(normalized_documents)} documents ({len(chunked_documents)} chunks so far)\")\n",
        "\n",
        "print(f\"Created {len(chunked_documents)} chunks from {len(normalized_documents)} documents\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extracting Transaction Entities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting entities from 156 chunks using ML (spaCy)...\n",
            "\n",
            "‚úÖ Extraction complete!\n",
            "   Total entities: 2420\n",
            "   Standard labels: ['CARDINAL', 'GPE', 'ORG', 'PERSON', 'FAC', 'MONEY', 'DATE', 'NORP', 'LOC', 'ORDINAL', 'PRODUCT', 'LAW', 'PERCENT', 'WORK_OF_ART', 'EVENT', 'TIME']\n",
            "   Transactions (filtered): 149\n",
            "   Wallets/Addresses (filtered): 10\n",
            "   Blocks (filtered): 398\n"
          ]
        }
      ],
      "source": [
        "from semantica.semantic_extract import NERExtractor\n",
        "\n",
        "# Initialize NERExtractor with ML method only (spaCy)\n",
        "# Note: ML method extracts standard NER labels (PERSON, ORG, GPE, MONEY, etc.)\n",
        "entity_extractor = NERExtractor(\n",
        "    method=[\"ml\"],\n",
        "    min_confidence=0.5\n",
        ")\n",
        "\n",
        "# Extract all entities using Semantica's extract() method - handles batch processing\n",
        "print(f\"Extracting entities from {len(chunked_documents)} chunks using ML (spaCy)...\")\n",
        "batch_results = entity_extractor.extract(chunked_documents)\n",
        "\n",
        "# Flatten results (extract() returns List[List[Entity]] for batch input)\n",
        "all_entities = [entity for entity_list in batch_results for entity in entity_list]\n",
        "\n",
        "# Use Semantica's classify_entities to group by standard labels\n",
        "classified = entity_extractor.classify_entities(all_entities)\n",
        "\n",
        "# Filter entities for blockchain transaction domain\n",
        "# Look for transaction hashes, wallet addresses, block numbers, and crypto tokens\n",
        "transaction_keywords = [\"transaction\", \"tx\", \"0x\", \"transfer\", \"sent\", \"received\"]\n",
        "wallet_keywords = [\"wallet\", \"address\", \"0x\", \"account\"]\n",
        "block_keywords = [\"block\", \"blockchain\", \"height\", \"block number\"]\n",
        "\n",
        "transactions = [\n",
        "    e for e in all_entities \n",
        "    if any(kw in e.text.lower() for kw in transaction_keywords) \n",
        "    or e.label == \"MONEY\"  # Money entities often represent transactions\n",
        "]\n",
        "wallets = [\n",
        "    e for e in all_entities \n",
        "    if any(kw in e.text.lower() for kw in wallet_keywords)\n",
        "    or (len(e.text) >= 26 and e.text.startswith(\"0x\"))  # Ethereum addresses\n",
        "]\n",
        "blocks = [\n",
        "    e for e in all_entities \n",
        "    if any(kw in e.text.lower() for kw in block_keywords)\n",
        "    or e.label == \"CARDINAL\"  # Block numbers are often cardinal numbers\n",
        "]\n",
        "\n",
        "print(f\"\\n‚úÖ Extraction complete!\")\n",
        "print(f\"   Total entities: {len(all_entities)}\")\n",
        "print(f\"   Standard labels: {list(classified.keys())}\")\n",
        "print(f\"   Transactions (filtered): {len(transactions)}\")\n",
        "print(f\"   Wallets/Addresses (filtered): {len(wallets)}\")\n",
        "print(f\"   Blocks (filtered): {len(blocks)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extracting Transaction Relationships\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting relationships from 156 chunks using ML (dependency parsing)...\n",
            "  Processed 20/156 chunks (36 relationships found)\n",
            "  Processed 40/156 chunks (102 relationships found)\n",
            "  Processed 60/156 chunks (169 relationships found)\n",
            "  Processed 80/156 chunks (215 relationships found)\n",
            "  Processed 100/156 chunks (361 relationships found)\n",
            "  Processed 120/156 chunks (436 relationships found)\n",
            "  Processed 140/156 chunks (570 relationships found)\n",
            "  Processed 156/156 chunks (648 relationships found)\n",
            "\n",
            "‚úÖ Extracted 648 relationships\n"
          ]
        }
      ],
      "source": [
        "from semantica.semantic_extract import RelationExtractor\n",
        "\n",
        "# Use ML-based dependency parsing to avoid rate limits\n",
        "relation_extractor = RelationExtractor(\n",
        "    method=\"dependency\",  # ML/NLP method - no API calls needed\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "all_relationships = []\n",
        "error_count = 0\n",
        "print(f\"Extracting relationships from {len(chunked_documents)} chunks using ML (dependency parsing)...\")\n",
        "\n",
        "for i, chunk in enumerate(chunked_documents, 1):\n",
        "    chunk_text = chunk.text if hasattr(chunk, 'text') else str(chunk)\n",
        "    try:\n",
        "        relationships = relation_extractor.extract_relations(\n",
        "            chunk_text,\n",
        "            entities=all_entities,\n",
        "            relation_types=[\"transfers\", \"from\", \"to\", \"in_block\", \"contains\", \"flows_to\"],\n",
        "            verbose=True\n",
        "        )\n",
        "        all_relationships.extend(relationships)\n",
        "    except Exception as e:\n",
        "        error_count += 1\n",
        "        if error_count <= 3:\n",
        "            print(f\"  Warning: Error on chunk {i}: {str(e)[:100]}\")\n",
        "    \n",
        "    if i % 20 == 0 or i == len(chunked_documents):\n",
        "        print(f\"  Processed {i}/{len(chunked_documents)} chunks ({len(all_relationships)} relationships found)\")\n",
        "\n",
        "if error_count > 0:\n",
        "    print(f\"  Note: {error_count} chunks had errors during relation extraction\")\n",
        "\n",
        "print(f\"\\n‚úÖ Extracted {len(all_relationships)} relationships\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detecting Transaction Conflicts\n",
        "\n",
        "-  **Multi-Type Detection**: Detects entity, relationship, and temporal conflicts across transaction network\n",
        "- **Most Recent Strategy**: Uses `most_recent` resolution for transaction data (most accurate for blockchain)\n",
        "- **Source-Aware Resolution**: Considers source reliability and confidence scores for conflict resolution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detecting conflicts in 2420 entities and 648 relationships...\n",
            "Detected 0 total conflicts\n",
            "‚úÖ No conflicts detected - data is consistent\n"
          ]
        }
      ],
      "source": [
        "from semantica.conflicts import ConflictDetector, ConflictResolver\n",
        "\n",
        "# Initialize conflict detection and resolution\n",
        "conflict_detector = ConflictDetector()\n",
        "conflict_resolver = ConflictResolver()\n",
        "\n",
        "# Use Semantica's conflict detection methods directly\n",
        "# Detects entity, relationship, and temporal conflicts\n",
        "print(f\"Detecting conflicts in {len(all_entities)} entities and {len(all_relationships)} relationships...\")\n",
        "\n",
        "# Convert to dict format for conflict detection (Semantica expects dicts)\n",
        "entity_dicts = [{\"id\": e.text, \"text\": e.text, \"type\": e.label, \"confidence\": getattr(e, 'confidence', 1.0)} for e in all_entities]\n",
        "relationship_dicts = [{\"id\": f\"{r.subject.text}_{r.predicate}_{r.object.text}\", \"source_id\": r.subject.text, \"target_id\": r.object.text, \"type\": r.predicate} for r in all_relationships]\n",
        "\n",
        "# Detect all conflict types using Semantica's methods\n",
        "all_conflicts = []\n",
        "all_conflicts.extend(conflict_detector.detect_entity_conflicts(entity_dicts))\n",
        "all_conflicts.extend(conflict_detector.detect_relationship_conflicts(relationship_dicts))\n",
        "all_conflicts.extend(conflict_detector.detect_temporal_conflicts(entity_dicts))\n",
        "\n",
        "print(f\"Detected {len(all_conflicts)} total conflicts\")\n",
        "\n",
        "# Resolve conflicts using best strategy for transaction networks\n",
        "if all_conflicts:\n",
        "    print(f\"Resolving conflicts using 'most_recent' strategy (best for transaction data)...\")\n",
        "    resolved = conflict_resolver.resolve_conflicts(all_conflicts, strategy=\"most_recent\")\n",
        "    resolved_count = len([r for r in resolved if r.resolved])\n",
        "    print(f\"‚úÖ Resolved {resolved_count}/{len(all_conflicts)} conflicts\")\n",
        "else:\n",
        "    print(\"‚úÖ No conflicts detected - data is consistent\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building Temporal Transaction Network Graph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building temporal transaction network graph...\n"
          ]
        }
      ],
      "source": [
        "from semantica.kg import GraphBuilder\n",
        "\n",
        "# Conflicts already resolved - disable expensive operations\n",
        "graph_builder = GraphBuilder(\n",
        "    merge_entities=False,  # Skip entity merging (already done in conflict resolution)\n",
        "    resolve_conflicts=False,  # Conflicts already resolved\n",
        "    entity_resolution_strategy=\"exact\",  \n",
        "    enable_temporal=True,\n",
        "    temporal_granularity=TEMPORAL_GRANULARITY,\n",
        "    track_history=True,  \n",
        "    version_snapshots=True \n",
        ")\n",
        "\n",
        "# Build graph - Semantica's build() method automatically shows progress and ETA\n",
        "kg_sources = [{\n",
        "    \"entities\": [{\"text\": e.text, \"type\": e.label, \"confidence\": getattr(e, 'confidence', 1.0)} for e in all_entities],\n",
        "    \"relationships\": [{\"source\": r.subject.text, \"target\": r.object.text, \"type\": r.predicate, \"confidence\": getattr(r, 'confidence', 1.0)} for r in all_relationships]\n",
        "}]\n",
        "\n",
        "kg = graph_builder.build(kg_sources)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generating Embeddings for Transactions and Wallets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.embeddings import EmbeddingGenerator\n",
        "\n",
        "embedding_gen = EmbeddingGenerator(\n",
        "    provider=\"sentence_transformers\",\n",
        "    model=EMBEDDING_MODEL\n",
        ")\n",
        "\n",
        "print(f\"Generating embeddings for {len(transactions)} transactions and {len(wallets)} wallets...\")\n",
        "transaction_texts = [t.text for t in transactions]\n",
        "transaction_embeddings = embedding_gen.generate_embeddings(transaction_texts)\n",
        "\n",
        "wallet_texts = [w.text for w in wallets]\n",
        "wallet_embeddings = embedding_gen.generate_embeddings(wallet_texts)\n",
        "\n",
        "print(f\"Generated {len(transaction_embeddings)} transaction embeddings and {len(wallet_embeddings)} wallet embeddings\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Populating Vector Store\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.vector_store import VectorStore\n",
        "\n",
        "vector_store = VectorStore(backend=\"faiss\", dimension=EMBEDDING_DIMENSION)\n",
        "\n",
        "print(f\"Storing {len(transaction_embeddings)} transaction vectors and {len(wallet_embeddings)} wallet vectors...\")\n",
        "transaction_ids = vector_store.store_vectors(\n",
        "    vectors=transaction_embeddings,\n",
        "    metadata=[{\"type\": \"transaction\", \"name\": t.text, \"label\": t.label} for t in transactions]\n",
        ")\n",
        "\n",
        "wallet_ids = vector_store.store_vectors(\n",
        "    vectors=wallet_embeddings,\n",
        "    metadata=[{\"type\": \"wallet\", \"name\": w.text, \"label\": w.label} for w in wallets]\n",
        ")\n",
        "\n",
        "print(f\"Stored {len(transaction_ids)} transaction vectors and {len(wallet_ids)} wallet vectors\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyzing Graph Structure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.kg import GraphAnalyzer, CentralityCalculator, CommunityDetector\n",
        "\n",
        "graph_analyzer = GraphAnalyzer()\n",
        "centrality_calc = CentralityCalculator()\n",
        "community_detector = CommunityDetector()\n",
        "\n",
        "analysis = graph_analyzer.analyze_graph(kg)\n",
        "\n",
        "degree_centrality = centrality_calc.calculate_degree_centrality(kg)\n",
        "betweenness_centrality = centrality_calc.calculate_betweenness_centrality(kg)\n",
        "closeness_centrality = centrality_calc.calculate_closeness_centrality(kg)\n",
        "\n",
        "communities = community_detector.detect_communities(kg, method=\"louvain\")\n",
        "connectivity = graph_analyzer.analyze_connectivity(kg)\n",
        "\n",
        "print(f\"Graph analytics:\")\n",
        "print(f\"  - Communities: {len(communities)}\")\n",
        "print(f\"  - Connected components: {len(connectivity.get('components', []))}\")\n",
        "print(f\"  - Graph density: {analysis.get('density', 0):.3f}\")\n",
        "print(f\"  - Central nodes (degree): {len(degree_centrality)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Temporal Graph Queries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.kg import TemporalGraphQuery\n",
        "\n",
        "temporal_query = TemporalGraphQuery(\n",
        "    enable_temporal_reasoning=True,\n",
        "    temporal_granularity=TEMPORAL_GRANULARITY\n",
        ")\n",
        "\n",
        "query_results = temporal_query.query_at_time(\n",
        "    kg,\n",
        "    query={\"type\": \"Transaction\"},\n",
        "    at_time=\"2024-01-01\"\n",
        ")\n",
        "\n",
        "evolution = temporal_query.analyze_evolution(kg)\n",
        "temporal_patterns = temporal_query.detect_temporal_patterns(kg, pattern_type=\"sequence\")\n",
        "\n",
        "print(f\"Temporal queries: {len(query_results)} transactions at query time\")\n",
        "print(f\"Temporal patterns detected: {len(temporal_patterns)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detecting Patterns and Whale Movements\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detect whale movements (large transactions)\n",
        "whale_wallets = []\n",
        "for entity in kg.get(\"entities\", []):\n",
        "    if entity.get(\"type\") in [\"Wallet\", \"Address\"]:\n",
        "        # Check for large transaction relationships\n",
        "        related_rels = [r for r in kg.get(\"relationships\", []) \n",
        "                        if r.get(\"source\") == entity.get(\"id\") or r.get(\"target\") == entity.get(\"id\")]\n",
        "        if any(\"large\" in str(r.get(\"type\", \"\")).lower() or \"whale\" in str(r.get(\"type\", \"\")).lower() \n",
        "               for r in related_rels):\n",
        "            whale_wallets.append(entity)\n",
        "\n",
        "# Detect suspicious patterns (high frequency transactions)\n",
        "suspicious_patterns = []\n",
        "for wallet in wallets[:10]:\n",
        "    wallet_name = wallet.text\n",
        "    paths = graph_analyzer.find_paths(\n",
        "        kg,\n",
        "        source=wallet_name,\n",
        "        target_type=\"Transaction\",\n",
        "        max_hops=1\n",
        "    )\n",
        "    if len(paths) > 5:  # High transaction frequency\n",
        "        suspicious_patterns.append({\n",
        "            'wallet': wallet_name,\n",
        "            'transaction_count': len(paths)\n",
        "        })\n",
        "\n",
        "print(f\"Whale tracking: {len(whale_wallets)} large transaction wallets identified\")\n",
        "print(f\"Suspicious patterns: {len(suspicious_patterns)} high-frequency wallets\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyzing Token Flows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze token flows through the network\n",
        "flow_analysis = []\n",
        "for transaction in transactions[:10]:\n",
        "    tx_name = transaction.text\n",
        "    paths = graph_analyzer.find_paths(\n",
        "        kg,\n",
        "        source=tx_name,\n",
        "        target_type=\"Wallet\",\n",
        "        max_hops=2\n",
        "    )\n",
        "    for path in paths:\n",
        "        if path.get('target_type') in ['Wallet', 'Address']:\n",
        "            flow_analysis.append({\n",
        "                'transaction': tx_name,\n",
        "                'flow_path': path.get('path', []),\n",
        "                'target': path.get('target'),\n",
        "                'path_length': len(path.get('path', []))\n",
        "            })\n",
        "\n",
        "flow_analysis.sort(key=lambda x: x['path_length'])\n",
        "\n",
        "print(f\"Flow analysis: {len(flow_analysis)} token flow paths identified\")\n",
        "for i, flow in enumerate(flow_analysis[:5], 1):\n",
        "    print(f\"{i}. {flow['transaction']} -> {flow['target']} (path length: {flow['path_length']})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Storing Transaction Network (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.graph_store import GraphStore\n",
        "\n",
        "# Optional: Store to persistent graph database\n",
        "# graph_store = GraphStore(backend=\"neo4j\", uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"password\")\n",
        "# graph_store.store_graph(kg)\n",
        "\n",
        "print(\"Graph store configured (commented out for demo)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GraphRAG: Hybrid Vector + Graph Queries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.context import AgentContext\n",
        "\n",
        "context = AgentContext(vector_store=vector_store, knowledge_graph=kg)\n",
        "\n",
        "query = \"What are the largest transactions?\"\n",
        "results = context.retrieve(\n",
        "    query,\n",
        "    max_results=10,\n",
        "    use_graph=True,\n",
        "    expand_graph=True,\n",
        "    include_entities=True,\n",
        "    include_relationships=True\n",
        ")\n",
        "\n",
        "print(f\"GraphRAG query: '{query}'\")\n",
        "print(f\"\\nRetrieved {len(results)} results:\\n\")\n",
        "for i, result in enumerate(results[:5], 1):\n",
        "    print(f\"{i}. Score: {result.get('score', 0):.3f}\")\n",
        "    print(f\"   Content: {result.get('content', '')[:200]}...\")\n",
        "    if result.get('related_entities'):\n",
        "        print(f\"   Related entities: {len(result['related_entities'])}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing the Transaction Network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.visualization import KGVisualizer\n",
        "\n",
        "visualizer = KGVisualizer()\n",
        "visualizer.visualize(\n",
        "    kg,\n",
        "    output_path=\"transaction_network.html\",\n",
        "    layout=\"hierarchical\",\n",
        "    node_size=20\n",
        ")\n",
        "\n",
        "print(\"Visualization saved to transaction_network.html\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exporting Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.export import GraphExporter\n",
        "\n",
        "exporter = GraphExporter()\n",
        "exporter.export(kg, output_path=\"transaction_network.json\", format=\"json\")\n",
        "exporter.export(kg, output_path=\"transaction_network.graphml\", format=\"graphml\")\n",
        "exporter.export(kg, output_path=\"transaction_network.csv\", format=\"csv\")\n",
        "\n",
        "print(\"Exported transaction network to JSON, GraphML, and CSV formats\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
