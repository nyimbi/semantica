{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Hawksight-AI/semantica/blob/main/cookbook/use_cases/trading/01_Risk_Assessment.ipynb)\n",
        "\n",
        "# Risk Assessment - Graph Analytics & Portfolio Risk Modeling\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook demonstrates **portfolio risk assessment** using Semantica with focus on **graph-based analytics**, **portfolio risk modeling**, **market simulation**, and **dependency analysis**. The pipeline assesses portfolio risk using graph-based analytics and market simulations.\n",
        "\n",
        "### Key Features\n",
        "\n",
        "- **Graph-Based Analytics**: Uses graph analytics for portfolio risk analysis\n",
        "- **Portfolio Risk Modeling**: Models portfolio relationships and dependencies\n",
        "- **Market Simulation**: Simulates market scenarios using graph data\n",
        "- **Dependency Analysis**: Analyzes dependencies between portfolio components\n",
        "- **Risk Modeling**: Emphasizes graph analytics, reasoning, and risk modeling\n",
        "\n",
        "### Pipeline Architecture\n",
        "\n",
        "1. **Phase 0**: Setup & Configuration\n",
        "2. **Phase 1**: Portfolio Data Ingestion\n",
        "3. **Phase 2**: Entity Extraction (Price, Signal, Pattern, Indicator, Strategy)\n",
        "4. **Phase 3**: Financial Knowledge Graph Construction\n",
        "5. **Phase 4**: Graph Analytics (Dependencies, Relationships)\n",
        "6. **Phase 5**: Portfolio Risk Modeling\n",
        "7. **Phase 6**: Market Simulation\n",
        "8. **Phase 7**: Visualization & Risk Reporting\n",
        "\n",
        "---\n",
        "\n",
        "## Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -qU semantica networkx matplotlib plotly pandas groq\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 0: Setup & Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from semantica.core import Semantica, ConfigManager\n",
        "from semantica.kg import GraphAnalytics\n",
        "from semantica.reasoning import GraphReasoner\n",
        "from semantica.graph_store import GraphStore\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\", \"your-key\")\n",
        "\n",
        "config_dict = {\n",
        "    \"project_name\": \"Risk_Assessment\",\n",
        "    \"extraction\": {\"provider\": \"groq\", \"model\": \"llama-3.1-8b-instant\"},\n",
        "    \"knowledge_graph\": {\"backend\": \"networkx\"}\n",
        "}\n",
        "\n",
        "config = ConfigManager().load_from_dict(config_dict)\n",
        "core = Semantica(config=config)\n",
        "print(\"Configured for risk assessment with graph analytics focus\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 1: Portfolio Data Ingestion\n",
        "\n",
        "Ingest portfolio data from databases, structured documents, and version-controlled configurations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.ingest import DBIngestor, RepoIngestor, FileIngestor\n",
        "from semantica.parse import DocumentParser\n",
        "from semantica.normalize import TextNormalizer\n",
        "from semantica.split import TextSplitter\n",
        "import os\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "documents = []\n",
        "\n",
        "# Option 1: Ingest from database (structured portfolio data)\n",
        "db_connection = \"sqlite:///data/portfolio.db\"\n",
        "try:\n",
        "    db_ingestor = DBIngestor()\n",
        "    # Create sample portfolio database structure\n",
        "    import sqlite3\n",
        "    conn = sqlite3.connect(\"data/portfolio.db\")\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS holdings (\n",
        "            id INTEGER PRIMARY KEY,\n",
        "            symbol TEXT,\n",
        "            allocation REAL,\n",
        "            sector TEXT,\n",
        "            risk_level TEXT\n",
        "        )\n",
        "    \"\"\")\n",
        "    cursor.execute(\"\"\"\n",
        "        INSERT OR REPLACE INTO holdings VALUES\n",
        "        (1, 'AAPL', 0.30, 'Technology', 'Medium'),\n",
        "        (2, 'MSFT', 0.25, 'Technology', 'Low'),\n",
        "        (3, 'GOOGL', 0.20, 'Technology', 'Medium'),\n",
        "        (4, 'BND', 0.25, 'Bonds', 'Low')\n",
        "    \"\"\")\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "    \n",
        "    db_data = db_ingestor.ingest_database(db_connection, method=\"sqlite\")\n",
        "    if db_data and \"data\" in db_data:\n",
        "        for table_data in db_data[\"data\"]:\n",
        "            documents.append(str(table_data))\n",
        "    print(f\"Ingested portfolio data from database\")\n",
        "except Exception as e:\n",
        "    print(f\"Database ingestion failed: {e}\")\n",
        "\n",
        "# Option 2: Parse structured portfolio documents (CSV/PDF)\n",
        "try:\n",
        "    portfolio_csv = \"\"\"symbol,allocation,sector,risk_level\n",
        "AAPL,0.30,Technology,Medium\n",
        "MSFT,0.25,Technology,Low\n",
        "GOOGL,0.20,Technology,Medium\n",
        "BND,0.25,Bonds,Low\"\"\"\n",
        "    with open(\"data/portfolio.csv\", \"w\") as f:\n",
        "        f.write(portfolio_csv)\n",
        "    \n",
        "    doc_parser = DocumentParser()\n",
        "    parsed_docs = doc_parser.parse(\"data/portfolio.csv\", file_type=\"csv\")\n",
        "    if parsed_docs:\n",
        "        documents.append(str(parsed_docs))\n",
        "    print(f\"Parsed portfolio document\")\n",
        "except Exception as e:\n",
        "    print(f\"Document parsing failed: {e}\")\n",
        "\n",
        "# Option 3: Ingest from version-controlled portfolio configs\n",
        "try:\n",
        "    portfolio_config = \"\"\"\n",
        "    Portfolio Configuration:\n",
        "    - AAPL: 30% allocation, Tech sector\n",
        "    - MSFT: 25% allocation, Tech sector  \n",
        "    - GOOGL: 20% allocation, Tech sector\n",
        "    - BND: 25% allocation, Bonds\n",
        "    Risk: High tech concentration (75%)\n",
        "    \"\"\"\n",
        "    os.makedirs(\"data/portfolio_config\", exist_ok=True)\n",
        "    with open(\"data/portfolio_config/config.txt\", \"w\") as f:\n",
        "        f.write(portfolio_config)\n",
        "    repo_docs = FileIngestor().ingest(\"data/portfolio_config\")\n",
        "    documents.extend(repo_docs)\n",
        "    print(f\"Ingested portfolio configuration from repository structure\")\n",
        "except Exception as e:\n",
        "    print(f\"Repository ingestion failed: {e}\")\n",
        "\n",
        "# Fallback: Sample data\n",
        "if not documents:\n",
        "    portfolio_data = \"\"\"\n",
        "    Portfolio contains: Stock AAPL (30%), Stock MSFT (25%), Stock GOOGL (20%), Bond BND (25%).\n",
        "    AAPL price correlates with tech sector performance.\n",
        "    MSFT depends on cloud services market growth.\n",
        "    Portfolio risk: High concentration in tech sector (75%).\n",
        "    Dependency: Tech sector downturn impacts 75% of portfolio.\n",
        "    \"\"\"\n",
        "    with open(\"data/portfolio.txt\", \"w\") as f:\n",
        "        f.write(portfolio_data)\n",
        "    documents = FileIngestor().ingest(\"data/portfolio.txt\")\n",
        "    print(f\"Ingested {len(documents)} documents from sample data\")\n",
        "\n",
        "# Normalize financial data\n",
        "normalizer = TextNormalizer()\n",
        "normalized_documents = []\n",
        "for doc in documents:\n",
        "    doc_content = doc.content if hasattr(doc, 'content') else str(doc)\n",
        "    normalized_text = normalizer.normalize(\n",
        "        doc_content,\n",
        "        clean_html=True,\n",
        "        normalize_entities=True,\n",
        "        normalize_numbers=True,\n",
        "        remove_extra_whitespace=True\n",
        "    )\n",
        "    normalized_documents.append(normalized_text)\n",
        "\n",
        "print(f\"Normalized {len(normalized_documents)} documents\")\n",
        "\n",
        "# Use token-based or recursive chunking for structured portfolio data\n",
        "splitter = TextSplitter(method=\"token\", chunk_size=1000, chunk_overlap=200)\n",
        "\n",
        "chunked_docs = []\n",
        "for doc_text in normalized_documents:\n",
        "    chunks = splitter.split(doc_text)\n",
        "    chunked_docs.extend([chunk.content if hasattr(chunk, 'content') else str(chunk) for chunk in chunks])\n",
        "\n",
        "print(f\"Created {len(chunked_docs)} token-based chunks\")\n",
        "print(\"Phase 1 complete: Portfolio data ingested from multiple structured sources\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.semantic_extract import NERExtractor, RelationExtractor, EventDetector\n",
        "\n",
        "# Extract entities using LLM-based NER\n",
        "ner_extractor = NERExtractor(method=\"llm\", provider=\"groq\", llm_model=\"llama-3.1-8b-instant\")\n",
        "entities = []\n",
        "for doc_text in chunked_docs:\n",
        "    extracted = ner_extractor.extract_entities(doc_text)\n",
        "    entities.extend(extracted)\n",
        "\n",
        "# Filter for financial entity types\n",
        "financial_entities = [\n",
        "    e for e in entities \n",
        "    if e.type in [\"Price\", \"Signal\", \"Pattern\", \"Indicator\", \"Strategy\"] or\n",
        "       any(keyword in e.text.lower() for keyword in [\"stock\", \"bond\", \"portfolio\", \"risk\", \"sector\"])\n",
        "]\n",
        "\n",
        "# Extract relationships\n",
        "relation_extractor = RelationExtractor(method=\"llm\", provider=\"groq\", llm_model=\"llama-3.1-8b-instant\")\n",
        "relationships = []\n",
        "for doc_text in chunked_docs:\n",
        "    extracted_rels = relation_extractor.extract_relations(doc_text, entities=financial_entities)\n",
        "    relationships.extend(extracted_rels)\n",
        "\n",
        "# Extract market events\n",
        "event_detector = EventDetector()\n",
        "events = []\n",
        "for doc_text in chunked_docs:\n",
        "    extracted_events = event_detector.detect_events(doc_text)\n",
        "    events.extend(extracted_events)\n",
        "\n",
        "print(f\"Extracted {len(financial_entities)} financial entities\")\n",
        "print(f\"Extracted {len(relationships)} relationships\")\n",
        "print(f\"Extracted {len(events)} market events\")\n",
        "print(\"Phase 2 complete: Entity extraction with NER, relations, and events\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 3: Financial Knowledge Graph Construction\n",
        "\n",
        "Build financial knowledge graph using GraphStore (Neo4j) and generate domain ontology.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.ontology import OntologyGenerator\n",
        "from semantica.context import AgentContext\n",
        "from semantica.vector_store import VectorStore\n",
        "\n",
        "# Build knowledge graph using core pipeline\n",
        "entity_dicts = [\n",
        "    {\"text\": e.text, \"type\": e.type, \"start\": e.start, \"end\": e.end, \"confidence\": getattr(e, 'confidence', 0.9)}\n",
        "    for e in financial_entities\n",
        "]\n",
        "\n",
        "relationship_dicts = [\n",
        "    {\n",
        "        \"subject\": rel.subject.text if hasattr(rel, 'subject') else str(rel.subject),\n",
        "        \"predicate\": rel.predicate if hasattr(rel, 'predicate') else \"related_to\",\n",
        "        \"object\": rel.object.text if hasattr(rel, 'object') else str(rel.object)\n",
        "    }\n",
        "    for rel in relationships\n",
        "]\n",
        "\n",
        "result = core.build_knowledge_base(\n",
        "    sources=chunked_docs,\n",
        "    custom_entity_types=[\"Price\", \"Signal\", \"Pattern\", \"Indicator\", \"Strategy\"],\n",
        "    graph=True,\n",
        "    embeddings=True\n",
        ")\n",
        "\n",
        "kg = result[\"knowledge_graph\"]\n",
        "print(f\"Built portfolio KG with {len(kg.get('entities', []))} entities\")\n",
        "\n",
        "# Generate financial domain ontology\n",
        "ontology_generator = OntologyGenerator(base_uri=\"https://semantica.dev/ontology/finance/\")\n",
        "ontology = ontology_generator.generate_ontology({\n",
        "    \"entities\": entity_dicts,\n",
        "    \"relationships\": relationship_dicts\n",
        "})\n",
        "\n",
        "print(f\"Generated ontology with {len(ontology.get('classes', []))} classes\")\n",
        "print(f\"Generated {len(ontology.get('properties', []))} properties\")\n",
        "\n",
        "# Setup GraphStore for persistent storage (Neo4j backend)\n",
        "# Note: For demo, we'll use networkx but show GraphStore usage\n",
        "try:\n",
        "    graph_store = GraphStore(backend=\"neo4j\", uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"password\")\n",
        "    # In production, you would store the graph here\n",
        "    # graph_store.create_node(labels=[\"Portfolio\"], properties={\"name\": \"Risk_Assessment\"})\n",
        "    print(\"GraphStore configured for Neo4j (connection skipped in demo)\")\n",
        "except Exception as e:\n",
        "    print(f\"GraphStore connection skipped (Neo4j not available): {e}\")\n",
        "\n",
        "# Setup GraphRAG for portfolio analysis\n",
        "vector_store = VectorStore(backend=\"faiss\", dimension=384)\n",
        "if result.get(\"embeddings\"):\n",
        "    vector_store.store_vectors(\n",
        "        vectors=result[\"embeddings\"][\"vectors\"],\n",
        "        metadata=result[\"embeddings\"][\"metadata\"]\n",
        "    )\n",
        "context = AgentContext(vector_store=vector_store, knowledge_graph=kg)\n",
        "\n",
        "print(\"Phase 3 complete: Financial KG constructed with ontology and GraphStore\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 4: Graph Analytics\n",
        "\n",
        "Perform comprehensive graph analytics: centrality analysis, community detection, and path analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform comprehensive graph analytics\n",
        "analytics = GraphAnalytics(kg)\n",
        "\n",
        "# Multiple centrality methods\n",
        "betweenness_centrality = analytics.calculate_centrality(method=\"betweenness\")\n",
        "degree_centrality = analytics.calculate_centrality(method=\"degree\")\n",
        "closeness_centrality = analytics.calculate_centrality(method=\"closeness\")\n",
        "eigenvector_centrality = analytics.calculate_centrality(method=\"eigenvector\")\n",
        "\n",
        "print(f\"Betweenness centrality: {len(betweenness_centrality)} nodes analyzed\")\n",
        "print(f\"Degree centrality: {len(degree_centrality)} nodes analyzed\")\n",
        "print(f\"Closeness centrality: {len(closeness_centrality)} nodes analyzed\")\n",
        "print(f\"Eigenvector centrality: {len(eigenvector_centrality)} nodes analyzed\")\n",
        "\n",
        "# Community detection for portfolio clusters\n",
        "try:\n",
        "    communities = analytics.detect_communities(method=\"louvain\")\n",
        "    print(f\"Detected {len(communities)} communities/clusters\")\n",
        "except Exception as e:\n",
        "    print(f\"Community detection: {e}\")\n",
        "\n",
        "# Path analysis for dependency chains\n",
        "try:\n",
        "    paths = analytics.find_paths(source=\"AAPL\", target=\"MSFT\", max_length=3)\n",
        "    print(f\"Found {len(paths)} paths between portfolio components\")\n",
        "except Exception as e:\n",
        "    print(f\"Path analysis: {e}\")\n",
        "\n",
        "# Use reasoning for dependency analysis\n",
        "reasoner = GraphReasoner(kg)\n",
        "dependencies = reasoner.find_patterns(pattern_type=\"dependency\")\n",
        "risk_patterns = reasoner.find_patterns(pattern_type=\"risk\")\n",
        "\n",
        "print(f\"Dependency analysis: {len(dependencies)} portfolio dependencies identified\")\n",
        "print(f\"Risk modeling: {len(risk_patterns)} risk patterns detected\")\n",
        "print(\"Phase 4 complete: Comprehensive graph analytics performed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Deduplication\n",
        "\n",
        "Resolve duplicate entities using graph-based clustering for risk assessment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.deduplication import ClusterBuilder, EntityMerger\n",
        "from semantica.semantic_extract import Entity\n",
        "\n",
        "# Convert Entity objects to dictionaries\n",
        "print(f\"Converting {len(financial_entities)} entities to dictionaries...\")\n",
        "entity_dicts = [{\"name\": e.text, \"type\": e.label, \"start_char\": e.start_char, \"end_char\": e.end_char, \"confidence\": e.confidence} for e in financial_entities]\n",
        "\n",
        "# Use graph_based clustering for risk assessment (identifies interconnected risks)\n",
        "# merge_all strategy combines all information from related risk entities\n",
        "cluster_builder = ClusterBuilder(method=\"graph_based\", similarity_threshold=0.85)\n",
        "\n",
        "print(f\"Building clusters for {len(entity_dicts)} entities using graph-based method...\")\n",
        "clusters = cluster_builder.build_clusters(entity_dicts)\n",
        "\n",
        "print(f\"Detected {len(clusters)} clusters\")\n",
        "print(f\"Merging entities within clusters using merge_all strategy...\")\n",
        "merger = EntityMerger()\n",
        "merged_entities_dicts = []\n",
        "for cluster in clusters:\n",
        "    if len(cluster) > 1:\n",
        "        # Merge entities within each cluster\n",
        "        merge_operations = merger.merge_duplicates(cluster, strategy=\"merge_all\", threshold=0.85)\n",
        "        if merge_operations:\n",
        "            merged_entities_dicts.extend([op.merged_entity for op in merge_operations])\n",
        "    else:\n",
        "        merged_entities_dicts.extend(cluster)\n",
        "\n",
        "# Convert back to Entity objects\n",
        "print(f\"Converting {len(merged_entities_dicts)} merged entities back to Entity objects...\")\n",
        "merged_entities = [\n",
        "    Entity(text=e[\"name\"], label=e[\"type\"], start_char=e.get(\"start_char\", 0), end_char=e.get(\"end_char\", 0), confidence=e.get(\"confidence\", 1.0))\n",
        "    for e in merged_entities_dicts\n",
        "]\n",
        "\n",
        "financial_entities = merged_entities\n",
        "print(f\"Deduplicated to {len(merged_entities)} unique entities\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 5: Portfolio Risk Modeling\n",
        "\n",
        "Detect conflicts, apply risk inference rules, and perform temporal graph analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.conflicts import ConflictDetector, ConflictResolver\n",
        "\n",
        "# Use logical conflict detection for portfolio risk rules\n",
        "# highest_confidence strategy prioritizes the most confident risk assessment\n",
        "conflict_detector = ConflictDetector()\n",
        "conflict_resolver = ConflictResolver()\n",
        "\n",
        "print(f\"Detecting logical conflicts in {len(financial_entities)} entities and relationships...\")\n",
        "conflicts = conflict_detector.detect_conflicts(\n",
        "    entities=financial_entities,\n",
        "    relationships=all_relationships,\n",
        "    method=\"logical\"  # Detect logical conflicts (e.g., conflicting risk indicators)\n",
        ")\n",
        "\n",
        "print(f\"Detected {len(conflicts)} logical conflicts\")\n",
        "\n",
        "if conflicts:\n",
        "    print(f\"Resolving conflicts using highest_confidence strategy...\")\n",
        "    resolved = conflict_resolver.resolve_conflicts(\n",
        "        conflicts,\n",
        "        strategy=\"highest_confidence\"  # Prioritize most confident risk assessment\n",
        "    )\n",
        "    print(f\"Resolved {len(resolved)} conflicts\")\n",
        "else:\n",
        "    print(\"No conflicts detected\")\n",
        "\n",
        "# Use reasoning for risk inference rules\n",
        "risk_rules = [\n",
        "    \"IF sector_concentration > 0.7 THEN high_risk\",\n",
        "    \"IF tech_sector_allocation > 0.5 AND bond_allocation < 0.3 THEN medium_risk\",\n",
        "    \"IF portfolio_diversity < 0.3 THEN high_risk\"\n",
        "]\n",
        "\n",
        "for rule in risk_rules:\n",
        "    reasoner.add_rule(rule)\n",
        "\n",
        "# Infer risk levels\n",
        "inferred_risks = reasoner.infer_facts(kg)\n",
        "print(f\"Inferred {len(inferred_risks)} risk facts from rules\")\n",
        "\n",
        "# Temporal graph analysis (if temporal data available)\n",
        "try:\n",
        "    temporal_kg = core.build_knowledge_base(\n",
        "        sources=chunked_docs,\n",
        "        custom_entity_types=[\"Price\", \"Signal\", \"Pattern\"],\n",
        "        graph=True,\n",
        "        temporal=True\n",
        "    )\n",
        "    print(\"Temporal knowledge graph constructed for time-series analysis\")\n",
        "except Exception as e:\n",
        "    print(f\"Temporal analysis: {e}\")\n",
        "\n",
        "print(\"Phase 5 complete: Risk modeling with conflict detection and inference\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 6: Market Simulation\n",
        "\n",
        "Export risk data, generate reports, and prepare data for external analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.export import ExportManager, ReportGenerator\n",
        "\n",
        "# Export to multiple formats\n",
        "export_manager = ExportManager()\n",
        "\n",
        "# Export to JSON\n",
        "export_manager.export_knowledge_graph(kg, \"data/portfolio_risk.json\", format=\"json\")\n",
        "print(\"Exported portfolio KG to JSON\")\n",
        "\n",
        "# Export to CSV\n",
        "export_manager.export_knowledge_graph(kg, \"data/portfolio_risk.csv\", format=\"csv\")\n",
        "print(\"Exported portfolio KG to CSV\")\n",
        "\n",
        "# Export to GraphML for external analysis\n",
        "export_manager.export_knowledge_graph(kg, \"data/portfolio_risk.graphml\", format=\"graphml\")\n",
        "print(\"Exported portfolio KG to GraphML\")\n",
        "\n",
        "# Export to RDF\n",
        "export_manager.export_knowledge_graph(kg, \"data/portfolio_risk.ttl\", format=\"rdf\", rdf_format=\"turtle\")\n",
        "print(\"Exported portfolio KG to RDF (Turtle)\")\n",
        "\n",
        "# Generate risk report\n",
        "report_generator = ReportGenerator()\n",
        "risk_report = report_generator.generate_report(\n",
        "    kg,\n",
        "    report_type=\"risk_assessment\",\n",
        "    output_path=\"data/risk_report.html\",\n",
        "    format=\"html\"\n",
        ")\n",
        "print(\"Generated HTML risk assessment report\")\n",
        "\n",
        "# Market simulation data preparation\n",
        "simulation_data = {\n",
        "    \"entities\": len(kg.get(\"entities\", [])),\n",
        "    \"relationships\": len(kg.get(\"relationships\", [])),\n",
        "    \"risk_patterns\": len(risk_patterns),\n",
        "    \"conflicts\": len(conflicts),\n",
        "    \"communities\": len(communities) if 'communities' in locals() else 0\n",
        "}\n",
        "\n",
        "print(f\"\\nMarket Simulation Data Summary:\")\n",
        "for key, value in simulation_data.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "print(\"Phase 6 complete: Market simulation data exported and reports generated\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 7: Visualization & Risk Reporting\n",
        "\n",
        "Visualize the knowledge graph and export to multiple formats.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.visualization import KGVisualizer\n",
        "\n",
        "# Visualize knowledge graph\n",
        "visualizer = KGVisualizer()\n",
        "visualizer.visualize(kg, output_path=\"portfolio_risk_kg.html\")\n",
        "\n",
        "print(\"Risk assessment analysis complete\")\n",
        "print(\"Emphasizes: Graph analytics, portfolio risk modeling, market simulation, dependency analysis\")\n",
        "print(\"\\nGenerated outputs:\")\n",
        "print(\"  - portfolio_risk_kg.html (visualization)\")\n",
        "print(\"  - data/portfolio_risk.json (JSON export)\")\n",
        "print(\"  - data/portfolio_risk.csv (CSV export)\")\n",
        "print(\"  - data/portfolio_risk.graphml (GraphML export)\")\n",
        "print(\"  - data/portfolio_risk.ttl (RDF export)\")\n",
        "print(\"  - data/risk_report.html (risk report)\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
