{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Hawksight-AI/semantica/blob/main/cookbook/use_cases/trading/02_News_Sentiment_Analysis.ipynb)\n",
        "\n",
        "# News Sentiment Analysis - Semantic Extraction & Correlation\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook demonstrates **news sentiment analysis** using Semantica with focus on **sentiment extraction**, **correlation analysis**, **financial KG**, and **price movement prediction**. The pipeline correlates news sentiment with price movements using financial knowledge graphs.\n",
        "\n",
        "### Key Features\n",
        "\n",
        "- **Sentiment Extraction**: Extracts sentiment from financial news articles\n",
        "- **Correlation Analysis**: Correlates sentiment with price movements\n",
        "- **Financial Knowledge Graph**: Builds financial KGs for correlation analysis\n",
        "- **Price Movement Prediction**: Predicts price movements based on sentiment\n",
        "- **Semantic Extraction**: Emphasizes semantic extraction and correlation analysis\n",
        "\n",
        "### Pipeline Architecture\n",
        "\n",
        "1. **Phase 0**: Setup & Configuration\n",
        "2. **Phase 1**: Financial News Ingestion\n",
        "3. **Phase 2**: Sentiment Extraction\n",
        "4. **Phase 3**: Financial Entity Extraction (Article, Sentiment, Price, Correlation, Trend)\n",
        "5. **Phase 4**: Financial Knowledge Graph Construction\n",
        "6. **Phase 5**: Sentiment-Price Correlation Analysis\n",
        "7. **Phase 6**: Price Movement Prediction\n",
        "8. **Phase 7**: Visualization & Export\n",
        "\n",
        "---\n",
        "\n",
        "## Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -qU semantica networkx matplotlib plotly pandas groq\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 0: Setup & Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from semantica.core import Semantica, ConfigManager\n",
        "from semantica.semantic_extract import SentimentAnalyzer\n",
        "from semantica.triplet_store import TripletStore\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\", \"your-key\")\n",
        "\n",
        "config_dict = {\n",
        "    \"project_name\": \"News_Sentiment_Analysis\",\n",
        "    \"extraction\": {\"provider\": \"groq\", \"model\": \"llama-3.1-8b-instant\"},\n",
        "    \"knowledge_graph\": {\"backend\": \"networkx\"}\n",
        "}\n",
        "\n",
        "config = ConfigManager().load_from_dict(config_dict)\n",
        "core = Semantica(config=config)\n",
        "sentiment_analyzer = SentimentAnalyzer()\n",
        "print(\"Configured for news sentiment analysis with semantic extraction focus\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 1: Financial News Ingestion\n",
        "\n",
        "Ingest financial news from streams, web sources, and structured APIs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.ingest import StreamIngestor, FileIngestor\n",
        "from semantica.parse import WebParser, StructuredDataParser\n",
        "from semantica.normalize import TextNormalizer\n",
        "from semantica.split import TextSplitter\n",
        "import os\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "documents = []\n",
        "\n",
        "# Option 1: Ingest from real-time news streams (simulated)\n",
        "try:\n",
        "    stream_ingestor = StreamIngestor()\n",
        "    # Simulate stream ingestion with sample data\n",
        "    stream_config = {\n",
        "        \"stream_type\": \"kafka\",\n",
        "        \"topics\": [\"financial_news\"],\n",
        "        \"bootstrap_servers\": \"localhost:9092\"\n",
        "    }\n",
        "    # In production, this would connect to actual stream\n",
        "    print(\"Stream ingestor configured (connection skipped in demo)\")\n",
        "except Exception as e:\n",
        "    print(f\"Stream ingestion setup: {e}\")\n",
        "\n",
        "# Option 2: Parse HTML news articles\n",
        "try:\n",
        "    web_parser = WebParser()\n",
        "    # Simulate web parsing with sample HTML\n",
        "    sample_html = \"\"\"\n",
        "    <html>\n",
        "        <head><title>Financial News</title></head>\n",
        "        <body>\n",
        "            <article>\n",
        "                <h1>AAPL Earnings Beat Expectations</h1>\n",
        "                <p>Apple Inc. reported strong quarterly earnings, exceeding analyst expectations.</p>\n",
        "            </article>\n",
        "        </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "    with open(\"data/sample_news.html\", \"w\") as f:\n",
        "        f.write(sample_html)\n",
        "    parsed_html = web_parser.parse_html(\"data/sample_news.html\")\n",
        "    if parsed_html:\n",
        "        documents.append(str(parsed_html))\n",
        "    print(\"Parsed HTML news article\")\n",
        "except Exception as e:\n",
        "    print(f\"Web parsing failed: {e}\")\n",
        "\n",
        "# Option 3: Parse structured JSON news APIs\n",
        "try:\n",
        "    structured_parser = StructuredDataParser()\n",
        "    news_json = {\n",
        "        \"articles\": [\n",
        "            {\n",
        "                \"title\": \"MSFT Faces Regulatory Concerns\",\n",
        "                \"content\": \"Microsoft faces new regulatory challenges in cloud services market.\",\n",
        "                \"date\": \"2024-01-02\",\n",
        "                \"sentiment\": \"negative\"\n",
        "            },\n",
        "            {\n",
        "                \"title\": \"GOOGL Announces New Product\",\n",
        "                \"content\": \"Google announces innovative new product line.\",\n",
        "                \"date\": \"2024-01-03\",\n",
        "                \"sentiment\": \"positive\"\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "    import json\n",
        "    with open(\"data/news.json\", \"w\") as f:\n",
        "        json.dump(news_json, f)\n",
        "    parsed_json = structured_parser.parse_json(\"data/news.json\")\n",
        "    if parsed_json:\n",
        "        documents.append(str(parsed_json))\n",
        "    print(\"Parsed structured JSON news data\")\n",
        "except Exception as e:\n",
        "    print(f\"Structured data parsing failed: {e}\")\n",
        "\n",
        "# Fallback: Sample data\n",
        "if not documents:\n",
        "    news_data = \"\"\"\n",
        "    2024-01-01: Positive news about AAPL earnings beat expectations. Stock price rose 5%.\n",
        "    2024-01-02: Negative sentiment: MSFT faces regulatory concerns. Stock price dropped 3%.\n",
        "    2024-01-03: Mixed sentiment: GOOGL announces new product. Stock price stable.\n",
        "    Correlation: Positive news correlates with price increases. Negative news correlates with price drops.\n",
        "    \"\"\"\n",
        "    with open(\"data/financial_news.txt\", \"w\") as f:\n",
        "        f.write(news_data)\n",
        "    documents = FileIngestor().ingest(\"data/financial_news.txt\")\n",
        "    print(f\"Ingested {len(documents)} documents from sample data\")\n",
        "\n",
        "# Normalize sentiment data\n",
        "normalizer = TextNormalizer()\n",
        "normalized_documents = []\n",
        "for doc in documents:\n",
        "    doc_content = doc.content if hasattr(doc, 'content') else str(doc)\n",
        "    normalized_text = normalizer.normalize(\n",
        "        doc_content,\n",
        "        clean_html=True,\n",
        "        normalize_entities=True,\n",
        "        remove_extra_whitespace=True\n",
        "    )\n",
        "    normalized_documents.append(normalized_text)\n",
        "\n",
        "print(f\"Normalized {len(normalized_documents)} documents\")\n",
        "\n",
        "# Use hierarchical or topic-based chunking for semantic article chunking\n",
        "splitter = TextSplitter(method=\"hierarchical\", chunk_size=1000, chunk_overlap=200)\n",
        "# Alternative: topic_based chunking\n",
        "# splitter = TextSplitter(method=\"topic_based\", chunk_size=1000)\n",
        "\n",
        "chunked_docs = []\n",
        "for doc_text in normalized_documents:\n",
        "    chunks = splitter.split(doc_text)\n",
        "    chunked_docs.extend([chunk.content if hasattr(chunk, 'content') else str(chunk) for chunk in chunks])\n",
        "\n",
        "print(f\"Created {len(chunked_docs)} hierarchical chunks\")\n",
        "print(\"Phase 1 complete: Financial news ingested from multiple sources\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract sentiment from news articles\n",
        "sentiments = []\n",
        "for doc_text in chunked_docs:\n",
        "    sentiment_result = sentiment_analyzer.analyze_sentiment(doc_text)\n",
        "    if isinstance(sentiment_result, list):\n",
        "        sentiments.extend(sentiment_result)\n",
        "    else:\n",
        "        sentiments.append(sentiment_result)\n",
        "\n",
        "print(f\"Extracted {len(sentiments)} sentiment scores\")\n",
        "\n",
        "# Extract sentiment per entity (if entities available)\n",
        "entity_sentiments = {}\n",
        "for i, doc_text in enumerate(chunked_docs):\n",
        "    sentiment = sentiment_analyzer.analyze_sentiment(doc_text)\n",
        "    # Store sentiment with document index\n",
        "    entity_sentiments[f\"doc_{i}\"] = sentiment\n",
        "\n",
        "# Emotion detection (if supported)\n",
        "try:\n",
        "    emotions = []\n",
        "    for doc_text in chunked_docs:\n",
        "        # SentimentAnalyzer may support emotion detection\n",
        "        emotion_result = sentiment_analyzer.analyze_sentiment(doc_text)\n",
        "        emotions.append(emotion_result)\n",
        "    print(f\"Detected emotions for {len(emotions)} documents\")\n",
        "except Exception as e:\n",
        "    print(f\"Emotion detection: {e}\")\n",
        "\n",
        "print(\"Phase 2 complete: Sentiment and emotion extraction performed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 3: Financial Entity Extraction\n",
        "\n",
        "Extract financial entities using spaCy-based NER and relation extraction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.semantic_extract import NERExtractor, RelationExtractor\n",
        "\n",
        "# Extract entities using spaCy-based NER (instead of LLM)\n",
        "ner_extractor = NERExtractor(method=\"ml\", model=\"en_core_web_sm\")\n",
        "entities = []\n",
        "for doc_text in chunked_docs:\n",
        "    extracted = ner_extractor.extract_entities(doc_text)\n",
        "    entities.extend(extracted)\n",
        "\n",
        "# Filter for financial entity types\n",
        "financial_entities = [\n",
        "    e for e in entities \n",
        "    if e.type in [\"Article\", \"Sentiment\", \"Price\", \"Correlation\", \"Trend\"] or\n",
        "       any(keyword in e.text.lower() for keyword in [\"stock\", \"price\", \"earnings\", \"news\", \"sentiment\"])\n",
        "]\n",
        "\n",
        "# Extract sentiment-price relationships\n",
        "relation_extractor = RelationExtractor(method=\"dependency\")\n",
        "relationships = []\n",
        "for doc_text in chunked_docs:\n",
        "    extracted_rels = relation_extractor.extract_relations(doc_text, entities=financial_entities)\n",
        "    relationships.extend(extracted_rels)\n",
        "\n",
        "# Build financial knowledge graph with sentiment\n",
        "result = core.build_knowledge_base(\n",
        "    sources=chunked_docs,\n",
        "    custom_entity_types=[\"Article\", \"Sentiment\", \"Price\", \"Correlation\", \"Trend\"],\n",
        "    graph=True,\n",
        "    embeddings=True\n",
        ")\n",
        "\n",
        "kg = result[\"knowledge_graph\"]\n",
        "print(f\"Built financial KG with {len(kg.get('entities', []))} entities\")\n",
        "print(f\"Extracted {len(financial_entities)} financial entities\")\n",
        "print(f\"Extracted {len(relationships)} sentiment-price relationships\")\n",
        "print(\"Phase 3 complete: Financial entity extraction with spaCy NER\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 4: Financial Knowledge Graph Construction\n",
        "\n",
        "Build temporal knowledge graph using TripletStore (RDF) and generate financial news ontology.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.ontology import OntologyGenerator\n",
        "from semantica.context import AgentContext\n",
        "from semantica.vector_store import VectorStore\n",
        "\n",
        "# Build temporal knowledge graph for time-series sentiment\n",
        "temporal_result = core.build_knowledge_base(\n",
        "    sources=chunked_docs,\n",
        "    custom_entity_types=[\"Article\", \"Sentiment\", \"Price\", \"Correlation\", \"Trend\"],\n",
        "    graph=True,\n",
        "    embeddings=True,\n",
        "    temporal=True\n",
        ")\n",
        "\n",
        "temporal_kg = temporal_result[\"knowledge_graph\"]\n",
        "print(f\"Built temporal KG with {len(temporal_kg.get('entities', []))} entities\")\n",
        "\n",
        "# Generate financial news ontology\n",
        "entity_dicts = [\n",
        "    {\"text\": e.text, \"type\": e.type, \"start\": e.start, \"end\": e.end}\n",
        "    for e in financial_entities\n",
        "]\n",
        "\n",
        "relationship_dicts = [\n",
        "    {\n",
        "        \"subject\": rel.subject.text if hasattr(rel, 'subject') else str(rel.subject),\n",
        "        \"predicate\": rel.predicate if hasattr(rel, 'predicate') else \"correlates_with\",\n",
        "        \"object\": rel.object.text if hasattr(rel, 'object') else str(rel.object)\n",
        "    }\n",
        "    for rel in relationships\n",
        "]\n",
        "\n",
        "ontology_generator = OntologyGenerator(base_uri=\"https://semantica.dev/ontology/finance/news/\")\n",
        "ontology = ontology_generator.generate_ontology({\n",
        "    \"entities\": entity_dicts,\n",
        "    \"relationships\": relationship_dicts\n",
        "})\n",
        "\n",
        "print(f\"Generated ontology with {len(ontology.get('classes', []))} classes\")\n",
        "print(f\"Generated {len(ontology.get('properties', []))} properties\")\n",
        "\n",
        "# Setup TripletStore for RDF storage\n",
        "try:\n",
        "    triplet_store = TripletStore(backend=\"rdflib\")\n",
        "    # Store triplets in RDF format\n",
        "    for rel in relationship_dicts:\n",
        "        triplet_store.add_triplet(\n",
        "            subject=rel[\"subject\"],\n",
        "            predicate=rel[\"predicate\"],\n",
        "            object=rel[\"object\"]\n",
        "        )\n",
        "    print(f\"Stored {len(relationship_dicts)} triplets in RDF store\")\n",
        "except Exception as e:\n",
        "    print(f\"TripletStore setup: {e}\")\n",
        "\n",
        "# Setup enhanced GraphRAG\n",
        "vector_store = VectorStore(backend=\"faiss\", dimension=384)\n",
        "if result.get(\"embeddings\"):\n",
        "    vector_store.store_vectors(\n",
        "        vectors=result[\"embeddings\"][\"vectors\"],\n",
        "        metadata=result[\"embeddings\"][\"metadata\"]\n",
        "    )\n",
        "context = AgentContext(vector_store=vector_store, knowledge_graph=kg)\n",
        "\n",
        "print(\"Phase 4 complete: Temporal KG constructed with TripletStore and ontology\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.reasoning import GraphReasoner\n",
        "from semantica.kg import GraphAnalytics\n",
        "\n",
        "# Use GraphReasoner with correlation rules\n",
        "reasoner = GraphReasoner(kg)\n",
        "correlations = reasoner.find_correlations(\n",
        "    source_types=[\"Sentiment\"],\n",
        "    target_types=[\"Price\"]\n",
        ")\n",
        "\n",
        "# Add correlation rules\n",
        "correlation_rules = [\n",
        "    \"IF sentiment = positive THEN price_change > 0\",\n",
        "    \"IF sentiment = negative THEN price_change < 0\",\n",
        "    \"IF sentiment_score > 0.7 THEN strong_positive_correlation\"\n",
        "]\n",
        "\n",
        "for rule in correlation_rules:\n",
        "    reasoner.add_rule(rule)\n",
        "\n",
        "# Identify sentiment-price correlations\n",
        "sentiment_price_rels = [\n",
        "    r for r in kg.get(\"relationships\", []) \n",
        "    if \"correlate\" in str(r.get(\"predicate\", \"\")).lower() or\n",
        "       \"sentiment\" in str(r.get(\"predicate\", \"\")).lower()\n",
        "]\n",
        "\n",
        "# Statistical correlation analysis\n",
        "import statistics\n",
        "sentiment_scores = [s.get(\"score\", 0) if isinstance(s, dict) else 0.5 for s in sentiments[:10]]\n",
        "if sentiment_scores:\n",
        "    avg_sentiment = statistics.mean(sentiment_scores)\n",
        "    print(f\"Average sentiment score: {avg_sentiment:.2f}\")\n",
        "\n",
        "# Use GraphAnalytics for pattern detection\n",
        "analytics = GraphAnalytics(kg)\n",
        "patterns = analytics.detect_patterns(pattern_type=\"correlation\")\n",
        "print(f\"Detected {len(patterns)} correlation patterns\")\n",
        "\n",
        "print(f\"Correlation analysis: {len(correlations)} sentiment-price correlations found\")\n",
        "print(f\"Sentiment-price relationships: {len(sentiment_price_rels)} relationships identified\")\n",
        "print(\"Phase 5 complete: Sentiment-price correlation analysis performed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 6: Price Movement Prediction\n",
        "\n",
        "Deduplicate news articles, use enhanced GraphRAG queries, and export predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.deduplication import DuplicateDetector, EntityMerger\n",
        "from semantica.deduplication.methods import detect_duplicates\n",
        "from semantica.semantic_extract import Entity\n",
        "from semantica.export import ExportManager\n",
        "\n",
        "# Convert Entity objects to dictionaries\n",
        "print(f\"Converting {len(financial_entities)} entities to dictionaries...\")\n",
        "entity_dicts = [{\"name\": e.text, \"type\": e.label, \"start_char\": e.start_char, \"end_char\": e.end_char, \"confidence\": e.confidence} for e in financial_entities]\n"
        "\n",
        "# Use batch method for news sentiment analysis (efficient for large datasets)\n",
        "# keep_most_complete strategy preserves all news article details\n",
        "print(f\"Detecting duplicates in {len(entity_dicts)} entities using batch method...\")\n",
        "duplicates = detect_duplicates(entity_dicts, method=\"batch\", similarity_threshold=0.85)\n",
        "\n",
        "print(f\"Detected {len(duplicates)} duplicate candidates\")\n",
        "print(f\"Merging duplicates using keep_most_complete strategy...\")\n",
        "merger = EntityMerger()\n",
        "merge_operations = merger.merge_duplicates(entity_dicts, strategy=\"keep_most_complete\", threshold=0.85)\n",
        "\n",
        "# Extract merged entities from merge operations\n",
        "if merge_operations:\n",
        "    resolved_entities = [op.merged_entity for op in merge_operations]\n",
        "    merged_ids = set()\n",
        "    for op in merge_operations:\n",
        "        for source in op.source_entities:\n",
        "            merged_ids.add(source.get(\"id\") or source.get(\"name\"))\n",
        "    for entity in entity_dicts:\n",
        "        entity_id = entity.get(\"id\") or entity.get(\"name\")\n",
        "        if entity_id not in merged_ids:\n",
        "            resolved_entities.append(entity)\n",
        "else:\n",
        "    resolved_entities = entity_dicts\n",
        "\n",
        "# Convert back to Entity objects\n",
        "print(f\"Converting {len(resolved_entities)} resolved entities back to Entity objects...\")\n",
        "merged_entities = [\n",
        "    Entity(text=e[\"name\"], label=e[\"type\"], start_char=e.get(\"start_char\", 0), end_char=e.get(\"end_char\", 0), confidence=e.get(\"confidence\", 1.0))\n",
        "    for e in resolved_entities\n",
        "]\n",
        "\n",
        "financial_entities = merged_entities\n",
        "print(f\"Deduplicated {len(entity_dicts)} entities to {len(merged_entities)} unique entities\")\n",
        "\n",
        "# Use AgentContext for enhanced GraphRAG queries\n",
        "query_results = context.query(\n",
        "    \"What is the correlation between positive sentiment and stock price increases?\",\n",
        "    top_k=5\n",
        ")\n",
        "print(f\"GraphRAG query returned {len(query_results) if query_results else 0} results\")\n",
        "\n",
        "# Price movement prediction based on sentiment\n",
        "predictions = []\n",
        "for i, sentiment in enumerate(sentiments[:5]):\n",
        "    if isinstance(sentiment, dict):\n",
        "        score = sentiment.get(\"score\", 0)\n",
        "    else:\n",
        "        score = 0.5\n",
        "    \n",
        "    if score > 0.6:\n",
        "        prediction = \"price_increase\"\n",
        "    elif score < 0.4:\n",
        "        prediction = \"price_decrease\"\n",
        "    else:\n",
        "        prediction = \"price_stable\"\n",
        "    \n",
        "    predictions.append({\n",
        "        \"article_id\": i,\n",
        "        \"sentiment_score\": score,\n",
        "        \"predicted_movement\": prediction\n",
        "    })\n",
        "\n",
        "print(f\"Generated {len(predictions)} price movement predictions\")\n",
        "\n",
        "# Export predictions\n",
        "export_manager = ExportManager()\n",
        "export_manager.export_knowledge_graph(\n",
        "    {\"predictions\": predictions},\n",
        "    \"data/price_predictions.json\",\n",
        "    format=\"json\"\n",
        ")\n",
        "print(\"Exported price predictions to JSON\")\n",
        "\n",
        "print(\"Phase 6 complete: Price movement prediction with deduplication\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conflict Detection\n",
        "\n",
        "Detect and resolve conflicts in sentiment classifications.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.conflicts import ConflictDetector, ConflictResolver\n",
        "\n",
        "# Use type conflict detection for conflicting sentiment classifications\n",
        "# voting strategy aggregates multiple sentiment analysis sources\n",
        "conflict_detector = ConflictDetector()\n",
        "conflict_resolver = ConflictResolver()\n",
        "\n",
        "print(f\"Detecting type conflicts in {len(financial_entities)} entities...\")\n",
        "conflicts = conflict_detector.detect_conflicts(\n",
        "    entities=financial_entities,\n",
        "    relationships=all_relationships,\n",
        "    method=\"type\"  # Detect conflicts in entity types/classifications (e.g., sentiment)\n",
        ")\n",
        "\n",
        "print(f\"Detected {len(conflicts)} type conflicts\")\n",
        "\n",
        "if conflicts:\n",
        "    print(f\"Resolving conflicts using voting strategy...\")\n",
        "    resolved = conflict_resolver.resolve_conflicts(\n",
        "        conflicts,\n",
        "        strategy=\"voting\"  # Majority vote from multiple sentiment sources\n",
        "    )\n",
        "    print(f\"Resolved {len(resolved)} conflicts\")\n",
        "else:\n",
        "    print(\"No conflicts detected\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 7: Visualization & Export\n",
        "\n",
        "Visualize the knowledge graph and export to RDF (Turtle), JSON-LD, and HTML reports.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from semantica.visualization import KGVisualizer\n",
        "from semantica.export import ReportGenerator\n",
        "\n",
        "# Visualize knowledge graph\n",
        "visualizer = KGVisualizer()\n",
        "visualizer.visualize(kg, output_path=\"news_sentiment_kg.html\")\n",
        "\n",
        "# Export to RDF (Turtle format)\n",
        "export_manager = ExportManager()\n",
        "export_manager.export_knowledge_graph(\n",
        "    kg,\n",
        "    \"data/news_sentiment.ttl\",\n",
        "    format=\"rdf\",\n",
        "    rdf_format=\"turtle\"\n",
        ")\n",
        "print(\"Exported KG to RDF (Turtle format)\")\n",
        "\n",
        "# Export to JSON-LD\n",
        "export_manager.export_knowledge_graph(\n",
        "    kg,\n",
        "    \"data/news_sentiment.jsonld\",\n",
        "    format=\"json-ld\"\n",
        ")\n",
        "print(\"Exported KG to JSON-LD\")\n",
        "\n",
        "# Generate HTML report\n",
        "report_generator = ReportGenerator()\n",
        "report = report_generator.generate_report(\n",
        "    kg,\n",
        "    report_type=\"sentiment_analysis\",\n",
        "    output_path=\"data/sentiment_report.html\",\n",
        "    format=\"html\"\n",
        ")\n",
        "print(\"Generated HTML sentiment analysis report\")\n",
        "\n",
        "print(\"News sentiment analysis complete\")\n",
        "print(\"Emphasizes: Sentiment extraction, correlation analysis, financial KG, price movement prediction\")\n",
        "print(\"\\nGenerated outputs:\")\n",
        "print(\"  - news_sentiment_kg.html (visualization)\")\n",
        "print(\"  - data/news_sentiment.ttl (RDF Turtle export)\")\n",
        "print(\"  - data/news_sentiment.jsonld (JSON-LD export)\")\n",
        "print(\"  - data/sentiment_report.html (HTML report)\")\n",
        "print(\"  - data/price_predictions.json (price predictions)\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
